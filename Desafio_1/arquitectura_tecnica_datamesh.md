# Documento de Arquitectura TÃ©cnica
## Data Mesh Platform - EspecificaciÃ³n de DiseÃ±o

**Cliente:** Orbidi  
**Fecha:** Diciembre 2025
**VersiÃ³n:** 2.0  
**Audiencia:** CTO, Data Engineering, Data Architecture

---

## Tabla de Contenidos

1. [Resumen TÃ©cnico](#1-resumen-tÃ©cnico)
2. [Arquitectura de Alto Nivel](#2-arquitectura-de-alto-nivel)
3. [Stack TecnolÃ³gico y ADRs](#3-stack-tecnolÃ³gico-y-adrs)
4. [Arquitectura Medallion](#4-arquitectura-medallion)
5. [DiseÃ±o de Pipelines](#5-diseÃ±o-de-pipelines)
6. [Modelo de Datos](#6-modelo-de-datos)
7. [Gobernanza y Seguridad](#7-gobernanza-y-seguridad)
8. [Observabilidad](#8-observabilidad)
9. [Infrastructure as Code](#9-infrastructure-as-code)
10. [Estrategia de Testing](#10-estrategia-de-testing)
11. [Performance y Escalabilidad](#11-performance-y-escalabilidad)
12. [EstimaciÃ³n de Costos Detallada](#12-estimaciÃ³n-de-costos-detallada)
13. [Riesgos y Mitigaciones](#13-riesgos-y-mitigaciones)
14. [Roadmap de ImplementaciÃ³n](#14-roadmap-de-implementaciÃ³n)

---

## 1. Resumen TÃ©cnico

### 1.1 Objetivos de Arquitectura

| Objetivo | MÃ©trica de Ã‰xito | Prioridad |
|----------|------------------|-----------|
| **Latencia de datos** | Datos en Gold < 4 horas desde origen | P0 |
| **Disponibilidad** | 99.9% uptime para capa Gold | P0 |
| **Escalabilidad** | Soportar 10x crecimiento sin rediseÃ±o | P1 |
| **Time-to-insight** | Nueva fuente productiva en < 5 dÃ­as | P1 |
| **Costo por TB** | < $50/TB/mes (storage + compute) | P2 |

### 1.2 Principios de DiseÃ±o

1. **Immutability en Bronze:** Los datos crudos nunca se modifican, solo se agregan
2. **Idempotencia:** Todos los pipelines pueden re-ejecutarse sin efectos secundarios
3. **Separation of Concerns:** Cada capa tiene una Ãºnica responsabilidad
4. **Schema-on-Read en Bronze, Schema-on-Write en Gold**
5. **Fail-Fast:** Detectar errores lo antes posible en el pipeline

### 1.3 Constraints TÃ©cnicos

- **Cloud Provider:** Google Cloud Platform (requisito contractual)
- **Open-Source Priority:** Minimizar vendor lock-in donde sea posible
- **Compliance:** GDPR-ready, datos PII identificados y protegidos
- **Budget:** Optimizar para volumen actual con capacidad de escalar

---

## 2. Arquitectura de Alto Nivel

### 2.1 Diagrama de Arquitectura Completo

![Diagrama de Arquitectura Completo](docs/diagrams/arquitectura_general.png)


### 2.2 Flujo de Datos End-to-End

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source  â”‚â”€â”€â”€â–¶â”‚  Airbyte â”‚â”€â”€â”€â–¶â”‚   GCS    â”‚â”€â”€â”€â–¶â”‚  Bronze  â”‚â”€â”€â”€â–¶â”‚  Silver  â”‚â”€â”€â”€â–¶â”‚   Gold   â”‚
â”‚          â”‚    â”‚  (E&L)   â”‚    â”‚  (Lake)  â”‚    â”‚  (Raw)   â”‚    â”‚ (Clean)  â”‚    â”‚ (Marts)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚               â”‚               â”‚               â”‚
     â”‚          CDC/API         Parquet          BigQuery        BigQuery        BigQuery
     â”‚          Incremental     Partitioned      Append-only     Incremental     Materialized
     â”‚                                                                               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    Lineage tracked in DataHub
```

---

## 3. Stack TecnolÃ³gico y ADRs

### 3.1 Matriz de TecnologÃ­as

| Capa | Componente | TecnologÃ­a | Licencia | Alternativa Descartada | JustificaciÃ³n |
|------|------------|------------|----------|------------------------|---------------|
| Ingesta | EL Tool | Airbyte OSS | MIT | Fivetran, Stitch | Open-source, sin costo por MAR |
| OrquestaciÃ³n | Workflow | Cloud Composer | Apache 2.0* | Airflow self-managed | Managed service, HA incluido |
| TransformaciÃ³n | ELT | dbt Core | Apache 2.0 | Dataform | MÃ¡s maduro, mejor comunidad |
| Warehouse | OLAP | BigQuery | Propietario | ClickHouse, Snowflake | Requisito GCP, mejor integraciÃ³n |
| CatÃ¡logo | Metadata | DataHub | Apache 2.0 | Data Catalog | Linaje automÃ¡tico, extensible |
| Calidad | Testing | Great Expectations | Apache 2.0 | Soda, Monte Carlo | Open-source, integraciÃ³n dbt |
| BI | Dashboards | Apache Superset | Apache 2.0 | Looker, Tableau | Open-source, SQL-native |
| ML | Platform | Vertex AI | Propietario | MLflow + Kubeflow | IntegraciÃ³n nativa GCP |
| IaC | Provisioning | Terraform | MPL 2.0 | Pulumi, CloudFormation | EstÃ¡ndar industria, multi-cloud |

*Cloud Composer es Airflow gestionado, el cÃ³digo de DAGs es 100% portable.

### 3.2 Architecture Decision Records (ADRs)

#### ADR-001: Airbyte sobre Fivetran para Ingesta

**Contexto:** Necesitamos una herramienta de Extract-Load con conectores para PostgreSQL, MySQL, MongoDB, SAP, Salesforce y SurveyMonkey.

**DecisiÃ³n:** Airbyte OSS desplegado en GKE.

**AnÃ¡lisis:**

| Criterio | Airbyte | Fivetran | Peso |
|----------|---------|----------|------|
| Licencia | MIT (Open-source) âœ… | Propietario âŒ | 30% |
| Conectores | 300+ âœ… | 150+ âœ… | 20% |
| Costo | Infra only (~$300/mo) âœ… | $1-2/MAR âŒ | 25% |
| Extensibilidad | Connector Builder âœ… | Limited âŒ | 15% |
| Ops overhead | Alto âš ï¸ | Bajo âœ… | 10% |

**Consecuencias:**
- (+) Cumple requisito open-source
- (+) Control total sobre sincronizaciones
- (+) Sin costo por volumen de datos
- (-) Mayor carga operacional (mitigaciÃ³n: runbooks, alertas)
- (-) Requiere GKE cluster dedicado

**Trade-off aceptado:** Overhead operacional a cambio de flexibilidad y costo predecible.

---

#### ADR-002: Cloud Composer sobre Airflow Self-Managed

**Contexto:** Necesitamos orquestaciÃ³n de pipelines con dependencias complejas.

**DecisiÃ³n:** Cloud Composer 2 (Airflow managed).

**AnÃ¡lisis:**

| Criterio | Composer | Self-managed | Peso |
|----------|----------|--------------|------|
| Ops overhead | Bajo âœ… | Alto âŒ | 35% |
| Escalabilidad | Auto-scaling âœ… | Manual âš ï¸ | 25% |
| IntegraciÃ³n GCP | Nativa âœ… | ConfiguraciÃ³n âš ï¸ | 20% |
| Portabilidad | DAGs portables âœ… | DAGs portables âœ… | 10% |
| Costo | ~$400/mo âš ï¸ | ~$200/mo âœ… | 10% |

**Consecuencias:**
- (+) HA y auto-scaling incluido
- (+) IntegraciÃ³n nativa con IAM, GCS, BigQuery
- (+) Actualizaciones de seguridad gestionadas
- (-) Costo fijo mayor
- (-) Menos control sobre versiones de Airflow

**Nota:** Los DAGs son 100% compatibles con Airflow OSS. Si se requiere migrar, el cÃ³digo es portable.

---

#### ADR-003: Arquitectura Medallion (Bronze/Silver/Gold)

**Contexto:** Necesitamos estructurar el data warehouse para soportar trazabilidad, reprocesamiento y diferentes niveles de acceso.

**DecisiÃ³n:** Implementar arquitectura Medallion con 3 capas.

**Diagrama de responsabilidades:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ARQUITECTURA MEDALLION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    CAPA     â”‚            RESPONSABILIDAD              â”‚       OWNER         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Bronze    â”‚ â€¢ Almacenar datos crudos (1:1 fuente)   â”‚ Data Engineering    â”‚
â”‚             â”‚ â€¢ Preservar historial completo          â”‚                     â”‚
â”‚             â”‚ â€¢ Schema-on-read                        â”‚                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Silver    â”‚ â€¢ Limpieza y deduplicaciÃ³n              â”‚ Data Engineering    â”‚
â”‚             â”‚ â€¢ Tipado y validaciÃ³n                   â”‚                     â”‚
â”‚             â”‚ â€¢ Modelos intermedios compartidos       â”‚                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Gold      â”‚ â€¢ Marts de negocio                      â”‚ Domain Teams        â”‚
â”‚             â”‚ â€¢ KPIs y agregaciones                   â”‚                     â”‚
â”‚             â”‚ â€¢ Optimizado para consumo               â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Consecuencias:**
- (+) Reprocesamiento desde Bronze si hay bugs
- (+) Debug simplificado (trazar Gold â†’ Silver â†’ Bronze)
- (+) Control de acceso por capa
- (-) Mayor almacenamiento (~2.1x datos originales)
- (-) Latencia adicional por transformaciones

**JustificaciÃ³n de costo:**

| Capa | % del total | Costo 100GB/mes |
|------|-------------|-----------------|
| Bronze | 48% | $2.00 |
| Silver | 38% | $1.60 |
| Gold | 14% | $0.60 |
| **Total** | 100% | **$4.20** |

El costo de storage es marginal ($0.02/GB/mes). El beneficio de trazabilidad justifica la duplicaciÃ³n.

---

#### ADR-004: DataHub sobre Google Data Catalog

**Contexto:** Necesitamos catalogaciÃ³n de datos con linaje automÃ¡tico.

**DecisiÃ³n:** DataHub OSS desplegado en GKE.

**ComparaciÃ³n:**

| Feature | DataHub | Data Catalog |
|---------|---------|--------------|
| Licencia | Apache 2.0 âœ… | Propietario âŒ |
| Linaje dbt | Nativo (manifest.json) âœ… | Manual âŒ |
| API | GraphQL extensible âœ… | REST limitado âš ï¸ |
| UI | Moderna, bÃºsqueda avanzada âœ… | BÃ¡sica âš ï¸ |
| Custom metadata | Aspectos personalizables âœ… | Tags simples âš ï¸ |

**Consecuencias:**
- (+) Linaje end-to-end automÃ¡tico
- (+) IntegraciÃ³n con dbt, Airflow, BigQuery
- (-) Requiere despliegue y mantenimiento

---

#### ADR-005: Datasets Gold Separados por Entidad

**Contexto:** Los marts de negocio deben servir a diferentes equipos con diferentes permisos.

**DecisiÃ³n:** Crear un dataset BigQuery por entidad de negocio (`gold_clientes`, `gold_productos`, `gold_ventas`).

**Alternativas consideradas:**

| OpciÃ³n | Pros | Cons |
|--------|------|------|
| Un solo dataset Gold | Simple | Sin granularidad de permisos |
| Un dataset por tabla | Muy granular | Demasiados datasets |
| **Un dataset por entidad** | Balance permisos/simplicidad âœ… | MÃ¡s datasets que opciÃ³n 1 |

**Consecuencias:**
- (+) Equipo de ventas solo ve `gold_ventas`
- (+) Billing granular por Ã¡rea de negocio
- (+) Quotas independientes por dataset
- (-) Cross-domain queries requieren permisos explÃ­citos

---

## 4. Arquitectura Medallion

### 4.1 Estructura de Datasets en BigQuery

```
ğŸ“ Proyecto GCP: maisons-data-platform
â”‚
â”œâ”€â”€ ğŸ¥‰ Dataset: bronze
â”‚   â”‚   Partitioned by: _airbyte_extracted_at (DAY)
â”‚   â”‚   Clustered by: _airbyte_source_id
â”‚   â”‚   Expiration: None (immutable)
â”‚   â”‚
â”‚   â”œâ”€â”€ raw_clientes          â† Salesforce (incremental, dedup)
â”‚   â”œâ”€â”€ raw_productos         â† SAP (full refresh daily)
â”‚   â”œâ”€â”€ raw_ventas            â† PostgreSQL (CDC)
â”‚   â”œâ”€â”€ raw_surveys           â† SurveyMonkey (incremental)
â”‚   â””â”€â”€ raw_inventario        â† MySQL (CDC)
â”‚
â”œâ”€â”€ ğŸ¥ˆ Dataset: silver
â”‚   â”‚   Partitioned by: loaded_at (DAY)
â”‚   â”‚   Clustered by: primary_key
â”‚   â”‚   Expiration: 365 days (configurable)
â”‚   â”‚
â”‚   â”œâ”€â”€ stg_clientes          â† Cleaned, typed, deduped
â”‚   â”œâ”€â”€ stg_productos         â† Normalized categories
â”‚   â”œâ”€â”€ stg_ventas            â† Joined with calendar
â”‚   â”œâ”€â”€ stg_surveys           â† Parsed responses
â”‚   â”œâ”€â”€ int_clientes_enriched â† With calculated fields
â”‚   â””â”€â”€ int_ventas_agg        â† Pre-aggregated for performance
â”‚
â”œâ”€â”€ ğŸ¥‡ Dataset: gold_clientes
â”‚   â”‚   Partitioned by: fecha_actualizacion (DAY)
â”‚   â”‚   Clustered by: segmento, region
â”‚   â”‚
â”‚   â”œâ”€â”€ dim_clientes          â† SCD Type 2
â”‚   â”œâ”€â”€ mart_clientes_360     â† Unified customer view
â”‚   â””â”€â”€ fct_interacciones     â† Customer touchpoints
â”‚
â”œâ”€â”€ ğŸ¥‡ Dataset: gold_productos
â”‚   â”‚   Partitioned by: fecha_actualizacion (DAY)
â”‚   â”‚   Clustered by: categoria, marca
â”‚   â”‚
â”‚   â”œâ”€â”€ dim_productos         â† Product dimension
â”‚   â”œâ”€â”€ mart_catalogo         â† Enriched catalog
â”‚   â””â”€â”€ fct_inventario        â† Inventory facts
â”‚
â””â”€â”€ ğŸ¥‡ Dataset: gold_ventas
    â”‚   Partitioned by: fecha_venta (DAY)
    â”‚   Clustered by: canal, region
    â”‚
    â”œâ”€â”€ fct_transacciones     â† Transaction facts
    â”œâ”€â”€ agg_ventas_diarias    â† Daily aggregations
    â””â”€â”€ mart_performance      â† Sales KPIs
```

### 4.2 Estrategia de MaterializaciÃ³n

| Capa | Modelo | MaterializaciÃ³n | JustificaciÃ³n |
|------|--------|-----------------|---------------|
| Bronze | `raw_*` | Table (append) | Airbyte escribe directamente |
| Silver | `stg_*` | Incremental | Solo procesar nuevos registros |
| Silver | `int_*` | Table | Modelos intermedios, reprocesar completo |
| Gold | `dim_*` | Table (SCD2) | Dimensiones con historial |
| Gold | `fct_*` | Incremental | Facts particionados |
| Gold | `mart_*` | Table | Agregaciones completas |
| Gold | `agg_*` | Incremental | Agregaciones incrementales |

### 4.3 Ejemplo de Modelo Incremental (Silver)

```sql
-- models/staging/stg_clientes.sql
{{
    config(
        materialized='incremental',
        unique_key='cliente_id',
        partition_by={
            'field': 'loaded_at',
            'data_type': 'timestamp',
            'granularity': 'day'
        },
        cluster_by=['segmento']
    )
}}

WITH source AS (
    SELECT
        id AS cliente_id,
        TRIM(LOWER(email)) AS email,
        INITCAP(nombre) AS nombre,
        INITCAP(apellido) AS apellido,
        telefono,
        CAST(fecha_registro AS TIMESTAMP) AS fecha_registro,
        COALESCE(segmento, 'standard') AS segmento,
        _airbyte_extracted_at AS loaded_at,
        _airbyte_ab_id AS airbyte_id
    FROM {{ source('bronze', 'raw_clientes') }}
    WHERE id IS NOT NULL
    {% if is_incremental() %}
        AND _airbyte_extracted_at > (SELECT MAX(loaded_at) FROM {{ this }})
    {% endif %}
),

deduplicated AS (
    SELECT *
    FROM source
    QUALIFY ROW_NUMBER() OVER (
        PARTITION BY cliente_id 
        ORDER BY loaded_at DESC
    ) = 1
)

SELECT * FROM deduplicated
```

### 4.4 Ejemplo de Modelo Gold (Mart)

```sql
-- models/marts/clientes/mart_clientes_360.sql
{{
    config(
        materialized='table',
        partition_by={
            'field': 'fecha_actualizacion',
            'data_type': 'date',
            'granularity': 'day'
        },
        cluster_by=['segmento', 'region']
    )
}}

WITH clientes AS (
    SELECT * FROM {{ ref('stg_clientes') }}
),

ventas AS (
    SELECT
        cliente_id,
        COUNT(DISTINCT venta_id) AS total_compras,
        SUM(monto) AS total_gastado,
        AVG(monto) AS ticket_promedio,
        MIN(fecha) AS primera_compra,
        MAX(fecha) AS ultima_compra,
        COUNT(DISTINCT DATE_TRUNC(fecha, MONTH)) AS meses_activo
    FROM {{ ref('stg_ventas') }}
    GROUP BY cliente_id
),

interacciones AS (
    SELECT
        cliente_id,
        COUNT(*) AS total_interacciones,
        COUNTIF(tipo = 'soporte') AS tickets_soporte,
        COUNTIF(tipo = 'nps') AS encuestas_respondidas
    FROM {{ ref('stg_interacciones') }}
    GROUP BY cliente_id
),

final AS (
    SELECT
        c.cliente_id,
        c.nombre,
        c.apellido,
        c.email,
        c.telefono,
        c.fecha_registro,
        
        -- MÃ©tricas de ventas
        COALESCE(v.total_compras, 0) AS total_compras,
        COALESCE(v.total_gastado, 0) AS total_gastado,
        COALESCE(v.ticket_promedio, 0) AS ticket_promedio,
        v.primera_compra,
        v.ultima_compra,
        DATE_DIFF(CURRENT_DATE(), v.ultima_compra, DAY) AS dias_desde_ultima_compra,
        
        -- MÃ©tricas de interacciÃ³n
        COALESCE(i.total_interacciones, 0) AS total_interacciones,
        COALESCE(i.tickets_soporte, 0) AS tickets_soporte,
        
        -- SegmentaciÃ³n
        CASE 
            WHEN v.total_gastado > 10000 THEN 'premium'
            WHEN v.total_gastado > 1000 THEN 'standard'
            ELSE 'basic'
        END AS segmento_valor,
        
        -- LTV Score (simplificado)
        ROUND(
            COALESCE(v.total_gastado, 0) * 
            (1 + COALESCE(v.meses_activo, 0) * 0.1) *
            (1 - LEAST(COALESCE(i.tickets_soporte, 0) * 0.05, 0.5)),
            2
        ) AS ltv_score,
        
        -- Metadata
        CURRENT_DATE() AS fecha_actualizacion,
        c.loaded_at AS source_loaded_at
        
    FROM clientes c
    LEFT JOIN ventas v ON c.cliente_id = v.cliente_id
    LEFT JOIN interacciones i ON c.cliente_id = i.cliente_id
)

SELECT * FROM final
```

---

## 5. DiseÃ±o de Pipelines

### 5.1 DAG Principal: Pipeline Multi-Entidad

```mermaid
flowchart TB
    subgraph Scheduler["â° Cloud Composer Scheduler"]
        CRON["0 6 * * * UTC (Daily 6 AM)"]
    end

    subgraph DAG["ğŸ”„ DAG: pipeline_maisons_daily"]
        direction TB
        
        subgraph Phase1["Phase 1: Ingesta (Paralelo)"]
            direction LR
            A1["sync_clientes<br/>Airbyte â†’ Bronze<br/>~10 min"]
            A2["sync_productos<br/>Airbyte â†’ Bronze<br/>~15 min"]
            A3["sync_ventas<br/>Airbyte â†’ Bronze<br/>~20 min"]
        end
        
        subgraph Phase2["Phase 2: Bronze â†’ Silver"]
            B["dbt_staging<br/>dbt run --select staging<br/>~5 min"]
        end
        
        subgraph Phase3["Phase 3: Silver â†’ Gold (Paralelo)"]
            direction LR
            C1["dbt_gold_clientes<br/>~8 min"]
            C2["dbt_gold_productos<br/>~6 min"]
            C3["dbt_gold_ventas<br/>~10 min"]
        end
        
        subgraph Phase4["Phase 4: Quality Gates"]
            D1["dbt_test<br/>~5 min"]
            D2["great_expectations<br/>~3 min"]
        end
        
        subgraph Phase5["Phase 5: Post-processing"]
            E1["update_datahub<br/>~2 min"]
            E2["refresh_cache<br/>~1 min"]
            E3["slack_notification"]
        end
    end

    CRON --> Phase1
    Phase1 --> Phase2
    Phase2 --> Phase3
    Phase3 --> Phase4
    Phase4 --> Phase5

    style A1 fill:#fce8e6,stroke:#ea4335
    style A2 fill:#fce8e6,stroke:#ea4335
    style A3 fill:#fce8e6,stroke:#ea4335
    style B fill:#e8f0fe,stroke:#4285f4
    style C1 fill:#e6f4ea,stroke:#34a853
    style C2 fill:#e6f4ea,stroke:#34a853
    style C3 fill:#e6f4ea,stroke:#34a853
    style D1 fill:#fff8e1,stroke:#fbc02d
    style D2 fill:#fff8e1,stroke:#fbc02d
```

### 5.2 CÃ³digo del DAG

```python
# dags/pipeline_maisons_daily.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.utils.task_group import TaskGroup

# Configuration
AIRBYTE_CONNECTIONS = {
    'clientes': 'conn_salesforce_clientes',
    'productos': 'conn_sap_productos',
    'ventas': 'conn_postgres_ventas',
}

DBT_PROJECT_DIR = '/home/airflow/gcs/dbt/maisons'
DBT_PROFILES_DIR = '/home/airflow/gcs/dbt/profiles'

default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'email_on_failure': True,
    'email': ['data-alerts@empresa.com'],
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'execution_timeout': timedelta(hours=2),
}

with DAG(
    dag_id='pipeline_maisons_daily',
    default_args=default_args,
    description='Pipeline diario de datos para dominio Maisons',
    schedule_interval='0 6 * * *',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_runs=1,
    tags=['maisons', 'production', 'daily'],
    doc_md="""
    ## Pipeline Maisons Daily
    
    Pipeline principal que procesa datos de todas las fuentes
    y los transforma a travÃ©s de las capas Bronze â†’ Silver â†’ Gold.
    
    ### SLA
    - Datos en Gold antes de 08:00 AM UTC
    - Tiempo mÃ¡ximo de ejecuciÃ³n: 2 horas
    
    ### Contacto
    - Slack: #data-engineering
    - PagerDuty: data-oncall
    """,
) as dag:

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 1: INGESTA (PARALELO)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    with TaskGroup(group_id='ingestion') as ingestion_group:
        ingestion_tasks = {}
        for entity, connection_id in AIRBYTE_CONNECTIONS.items():
            ingestion_tasks[entity] = AirbyteTriggerSyncOperator(
                task_id=f'sync_{entity}',
                connection_id=connection_id,
                asynchronous=False,
                timeout=3600,
                wait_seconds=30,
            )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 2: BRONZE â†’ SILVER
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    dbt_staging = BashOperator(
        task_id='dbt_staging',
        bash_command=f'''
            cd {DBT_PROJECT_DIR} && \
            dbt run \
                --select staging \
                --target prod \
                --profiles-dir {DBT_PROFILES_DIR} \
                --vars '{{"run_date": "{{{{ ds }}}}"}}' \
                2>&1 | tee /tmp/dbt_staging.log
        ''',
        pool='dbt_pool',
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 3: SILVER â†’ GOLD (PARALELO)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    with TaskGroup(group_id='marts') as marts_group:
        dbt_gold_clientes = BashOperator(
            task_id='dbt_gold_clientes',
            bash_command=f'''
                cd {DBT_PROJECT_DIR} && \
                dbt run \
                    --select marts.clientes \
                    --target prod \
                    --profiles-dir {DBT_PROFILES_DIR}
            ''',
            pool='dbt_pool',
        )
        
        dbt_gold_productos = BashOperator(
            task_id='dbt_gold_productos',
            bash_command=f'''
                cd {DBT_PROJECT_DIR} && \
                dbt run \
                    --select marts.productos \
                    --target prod \
                    --profiles-dir {DBT_PROFILES_DIR}
            ''',
            pool='dbt_pool',
        )
        
        dbt_gold_ventas = BashOperator(
            task_id='dbt_gold_ventas',
            bash_command=f'''
                cd {DBT_PROJECT_DIR} && \
                dbt run \
                    --select marts.ventas \
                    --target prod \
                    --profiles-dir {DBT_PROFILES_DIR}
            ''',
            pool='dbt_pool',
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 4: QUALITY GATES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    with TaskGroup(group_id='quality') as quality_group:
        dbt_test = BashOperator(
            task_id='dbt_test',
            bash_command=f'''
                cd {DBT_PROJECT_DIR} && \
                dbt test \
                    --select marts \
                    --target prod \
                    --profiles-dir {DBT_PROFILES_DIR}
            ''',
            pool='dbt_pool',
        )
        
        great_expectations = BashOperator(
            task_id='great_expectations',
            bash_command='''
                cd /home/airflow/gcs/great_expectations && \
                great_expectations checkpoint run gold_checkpoint
            ''',
        )
        
        dbt_test >> great_expectations

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 5: POST-PROCESSING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    with TaskGroup(group_id='post_processing') as post_group:
        update_datahub = BashOperator(
            task_id='update_datahub',
            bash_command=f'''
                cd {DBT_PROJECT_DIR} && \
                datahub ingest -c /home/airflow/gcs/datahub/dbt_recipe.yaml
            ''',
        )
        
        refresh_superset_cache = BashOperator(
            task_id='refresh_superset_cache',
            bash_command='''
                curl -X POST \
                    -H "Authorization: Bearer $SUPERSET_TOKEN" \
                    "$SUPERSET_URL/api/v1/chart/warm_up_cache"
            ''',
        )
        
        slack_notification = SlackWebhookOperator(
            task_id='slack_notification',
            webhook_token='{{ var.value.slack_data_webhook }}',
            message='''
:white_check_mark: *Pipeline Maisons Daily - SUCCESS*
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
:calendar: Fecha: {{ ds }}
:stopwatch: DuraciÃ³n: {{ ti.xcom_pull(task_ids='dbt_test', key='duration') }}
:chart_with_upwards_trend: Registros procesados:
â€¢ Clientes: {{ ti.xcom_pull(task_ids='dbt_gold_clientes', key='rows') }}
â€¢ Productos: {{ ti.xcom_pull(task_ids='dbt_gold_productos', key='rows') }}
â€¢ Ventas: {{ ti.xcom_pull(task_ids='dbt_gold_ventas', key='rows') }}
:white_check_mark: Tests: {{ ti.xcom_pull(task_ids='dbt_test', key='tests_passed') }} passed
:link: <{{ ti.log_url }}|Ver logs en Airflow>
            ''',
            channel='#data-alerts',
            trigger_rule='all_success',
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DEPENDENCIES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ingestion_group >> dbt_staging >> marts_group >> quality_group >> post_group
```

### 5.3 Timeline de EjecuciÃ³n

```
06:00 â”€â”¬â”€ sync_clientes â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (10 min)
       â”œâ”€ sync_productos â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (15 min)
       â””â”€ sync_ventas â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (20 min)
06:20 â”€â”€â”€ dbt_staging â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (5 min)
06:25 â”€â”¬â”€ dbt_gold_clientes â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (8 min)
       â”œâ”€ dbt_gold_productos â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (6 min)
       â””â”€ dbt_gold_ventas â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (10 min)
06:35 â”€â”€â”€ dbt_test â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (5 min)
06:40 â”€â”€â”€ great_expectations â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (3 min)
06:43 â”€â”¬â”€ update_datahub â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (2 min)
       â”œâ”€ refresh_cache â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (1 min)
       â””â”€ slack_notification (instant)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TOTAL: ~45 minutos
SLA: Datos en Gold antes de 08:00 AM âœ…
Buffer: ~75 minutos para retries
```

---

## 6. Modelo de Datos

### 6.1 Diagrama Entidad-RelaciÃ³n (Gold Layer)

```mermaid
erDiagram
    dim_clientes ||--o{ fct_transacciones : "cliente_id"
    dim_clientes ||--o{ fct_interacciones : "cliente_id"
    dim_productos ||--o{ fct_transacciones : "producto_id"
    dim_productos ||--o{ fct_inventario : "producto_id"
    dim_fecha ||--o{ fct_transacciones : "fecha_id"
    dim_fecha ||--o{ agg_ventas_diarias : "fecha_id"
    
    dim_clientes {
        string cliente_id PK
        string nombre
        string apellido
        string email
        string telefono
        string segmento
        date fecha_registro
        date valid_from
        date valid_to
        boolean is_current
    }
    
    dim_productos {
        string producto_id PK
        string nombre
        string categoria
        string subcategoria
        string marca
        decimal precio_lista
        boolean is_active
    }
    
    dim_fecha {
        date fecha_id PK
        int anio
        int mes
        int dia
        string dia_semana
        boolean es_feriado
        int semana_fiscal
    }
    
    fct_transacciones {
        string transaccion_id PK
        string cliente_id FK
        string producto_id FK
        date fecha_id FK
        decimal monto
        int cantidad
        string canal
        string metodo_pago
    }
    
    fct_interacciones {
        string interaccion_id PK
        string cliente_id FK
        date fecha_id FK
        string tipo
        string canal
        int duracion_seg
        int score_nps
    }
    
    fct_inventario {
        string producto_id FK
        date fecha_id FK
        int stock_disponible
        int stock_reservado
        int dias_cobertura
    }
    
    agg_ventas_diarias {
        date fecha_id FK
        string canal
        string region
        decimal total_ventas
        int num_transacciones
        int num_clientes_unicos
    }
    
    mart_clientes_360 {
        string cliente_id PK
        string nombre_completo
        string segmento_valor
        decimal ltv_score
        int total_compras
        decimal total_gastado
        date ultima_compra
        int dias_desde_ultima_compra
    }
```

### 6.2 Estructura de Proyecto dbt

```
dbt_maisons/
â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ packages.yml
â”œâ”€â”€ profiles.yml.example
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/                          # â†’ Dataset: silver
â”‚   â”‚   â”œâ”€â”€ _staging__sources.yml         # Source definitions
â”‚   â”‚   â”œâ”€â”€ _staging__models.yml          # Model configs & tests
â”‚   â”‚   â”œâ”€â”€ stg_clientes.sql
â”‚   â”‚   â”œâ”€â”€ stg_productos.sql
â”‚   â”‚   â”œâ”€â”€ stg_ventas.sql
â”‚   â”‚   â””â”€â”€ stg_surveys.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ intermediate/                     # â†’ Dataset: silver
â”‚   â”‚   â”œâ”€â”€ _int__models.yml
â”‚   â”‚   â”œâ”€â”€ int_clientes_enriched.sql
â”‚   â”‚   â”œâ”€â”€ int_productos_categorized.sql
â”‚   â”‚   â””â”€â”€ int_ventas_aggregated.sql
â”‚   â”‚
â”‚   â””â”€â”€ marts/                            # â†’ Datasets: gold_*
â”‚       â”œâ”€â”€ clientes/                     # â†’ gold_clientes
â”‚       â”‚   â”œâ”€â”€ _clientes__models.yml
â”‚       â”‚   â”œâ”€â”€ dim_clientes.sql
â”‚       â”‚   â”œâ”€â”€ mart_clientes_360.sql
â”‚       â”‚   â””â”€â”€ fct_interacciones.sql
â”‚       â”‚
â”‚       â”œâ”€â”€ productos/                    # â†’ gold_productos
â”‚       â”‚   â”œâ”€â”€ _productos__models.yml
â”‚       â”‚   â”œâ”€â”€ dim_productos.sql
â”‚       â”‚   â”œâ”€â”€ mart_catalogo.sql
â”‚       â”‚   â””â”€â”€ fct_inventario.sql
â”‚       â”‚
â”‚       â”œâ”€â”€ ventas/                       # â†’ gold_ventas
â”‚       â”‚   â”œâ”€â”€ _ventas__models.yml
â”‚       â”‚   â”œâ”€â”€ fct_transacciones.sql
â”‚       â”‚   â”œâ”€â”€ agg_ventas_diarias.sql
â”‚       â”‚   â””â”€â”€ mart_performance.sql
â”‚       â”‚
â”‚       â””â”€â”€ shared/                       # â†’ gold_shared
â”‚           â”œâ”€â”€ dim_fecha.sql
â”‚           â””â”€â”€ dim_geografia.sql
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ generic/
â”‚   â”‚   â””â”€â”€ test_positive_values.sql
â”‚   â””â”€â”€ singular/
â”‚       â”œâ”€â”€ test_ventas_sin_cliente.sql
â”‚       â””â”€â”€ test_fechas_futuras.sql
â”‚
â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ generate_schema_name.sql
â”‚   â”œâ”€â”€ incremental_strategy.sql
â”‚   â””â”€â”€ data_quality.sql
â”‚
â”œâ”€â”€ seeds/
â”‚   â”œâ”€â”€ dim_fecha.csv
â”‚   â””â”€â”€ categorias_mapping.csv
â”‚
â””â”€â”€ snapshots/
    â””â”€â”€ snap_clientes.sql                 # SCD Type 2
```

### 6.3 ConfiguraciÃ³n de Schemas (generate_schema_name.sql)

```sql
-- macros/generate_schema_name.sql
{% macro generate_schema_name(custom_schema_name, node) -%}
    {%- set default_schema = target.schema -%}
    
    {%- if custom_schema_name is none -%}
        {{ default_schema }}
    
    {%- elif custom_schema_name.startswith('gold_') -%}
        {# Gold models go to their specific dataset #}
        {{ custom_schema_name }}
    
    {%- elif node.resource_type == 'model' and node.fqn[1] == 'staging' -%}
        {# Staging models go to silver #}
        silver
    
    {%- elif node.resource_type == 'model' and node.fqn[1] == 'intermediate' -%}
        {# Intermediate models go to silver #}
        silver
    
    {%- else -%}
        {{ default_schema }}_{{ custom_schema_name | trim }}
    
    {%- endif -%}
{%- endmacro %}
```

---

## 7. Gobernanza y Seguridad

### 7.1 Modelo de Control de Acceso (IAM)

```yaml
# terraform/iam.tf (representaciÃ³n YAML para documentaciÃ³n)

# Grupos de usuarios
groups:
  - name: grp-data-engineering
    description: "Equipo de ingenierÃ­a de datos"
    members:
      - user:engineer1@empresa.com
      - user:engineer2@empresa.com
    
  - name: grp-data-analysts
    description: "Analistas de datos"
    members:
      - user:analyst1@empresa.com
      - user:analyst2@empresa.com
      
  - name: grp-domain-clientes
    description: "Equipo dominio clientes"
    members:
      - group:grp-data-analysts
      - user:pm-clientes@empresa.com

# Bindings por dataset
dataset_bindings:
  bronze:
    - role: roles/bigquery.dataEditor
      members:
        - serviceAccount:airbyte-sa@project.iam.gserviceaccount.com
    - role: roles/bigquery.dataViewer
      members:
        - group:grp-data-engineering

  silver:
    - role: roles/bigquery.dataEditor
      members:
        - serviceAccount:dbt-sa@project.iam.gserviceaccount.com
    - role: roles/bigquery.dataViewer
      members:
        - group:grp-data-engineering
        - group:grp-data-scientists

  gold_clientes:
    - role: roles/bigquery.dataViewer
      members:
        - group:grp-domain-clientes
        - group:grp-data-engineering
    - role: roles/bigquery.dataEditor
      members:
        - serviceAccount:dbt-sa@project.iam.gserviceaccount.com

  gold_productos:
    - role: roles/bigquery.dataViewer
      members:
        - group:grp-domain-productos
        - group:grp-data-engineering

  gold_ventas:
    - role: roles/bigquery.dataViewer
      members:
        - group:grp-domain-ventas
        - group:grp-data-engineering
```

### 7.2 Column-Level Security

```sql
-- Crear taxonomÃ­a para clasificaciÃ³n de datos
CREATE SCHEMA IF NOT EXISTS `maisons-data-platform.taxonomy`;

-- Policy tag para PII
CREATE POLICY TAG `maisons-data-platform.taxonomy.pii`
  DESCRIPTION 'InformaciÃ³n Personal Identificable - Requiere aprobaciÃ³n especial';

-- Policy tag para datos financieros
CREATE POLICY TAG `maisons-data-platform.taxonomy.financial`
  DESCRIPTION 'Datos financieros sensibles';

-- Aplicar tags a columnas sensibles
ALTER TABLE `gold_clientes.mart_clientes_360`
  ALTER COLUMN email SET POLICY TAG `maisons-data-platform.taxonomy.pii`,
  ALTER COLUMN telefono SET POLICY TAG `maisons-data-platform.taxonomy.pii`;

ALTER TABLE `gold_clientes.dim_clientes`
  ALTER COLUMN email SET POLICY TAG `maisons-data-platform.taxonomy.pii`,
  ALTER COLUMN telefono SET POLICY TAG `maisons-data-platform.taxonomy.pii`;

ALTER TABLE `gold_ventas.fct_transacciones`
  ALTER COLUMN monto SET POLICY TAG `maisons-data-platform.taxonomy.financial`;

-- Otorgar acceso a PII solo a grupo especÃ­fico
GRANT `roles/datacatalog.fineGrainedReader`
  ON POLICY TAG `maisons-data-platform.taxonomy.pii`
  TO 'group:grp-pii-access@empresa.com';

-- Otorgar acceso a datos financieros
GRANT `roles/datacatalog.fineGrainedReader`
  ON POLICY TAG `maisons-data-platform.taxonomy.financial`
  TO 'group:grp-finance@empresa.com';
```

### 7.3 Service Accounts y Workload Identity

```yaml
# Service Accounts
service_accounts:
  - name: airbyte-sa
    description: "Airbyte ingestion service"
    roles:
      - roles/bigquery.dataEditor (bronze)
      - roles/storage.objectCreator (gs://maisons-data-lake)
    workload_identity:
      namespace: airbyte
      service_account: airbyte-server

  - name: dbt-sa
    description: "dbt transformation service"
    roles:
      - roles/bigquery.dataViewer (bronze)
      - roles/bigquery.dataEditor (silver, gold_*)
      - roles/bigquery.jobUser
    workload_identity:
      namespace: airflow
      service_account: airflow-worker

  - name: superset-sa
    description: "Superset BI service"
    roles:
      - roles/bigquery.dataViewer (gold_*)
      - roles/bigquery.jobUser
    workload_identity:
      namespace: superset
      service_account: superset-server

  - name: datahub-sa
    description: "DataHub metadata service"
    roles:
      - roles/bigquery.metadataViewer
      - roles/datacatalog.viewer
```

### 7.4 AuditorÃ­a y Compliance

```sql
-- Vista de auditorÃ­a de accesos a datos sensibles
CREATE OR REPLACE VIEW `audit.vw_pii_access_log` AS
SELECT
  protopayload_auditlog.authenticationInfo.principalEmail AS user_email,
  resource.labels.dataset_id AS dataset,
  resource.labels.table_id AS table,
  protopayload_auditlog.methodName AS action,
  timestamp,
  protopayload_auditlog.requestMetadata.callerIp AS ip_address
FROM `maisons-data-platform.audit_logs._AllLogs`
WHERE
  resource.type = 'bigquery_dataset'
  AND protopayload_auditlog.methodName LIKE '%tables.get%'
  AND resource.labels.dataset_id IN ('gold_clientes', 'gold_ventas')
  AND DATE(timestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)
ORDER BY timestamp DESC;
```

---

## 8. Observabilidad

### 8.1 MÃ©tricas y Dashboards

```yaml
# Cloud Monitoring - Custom Metrics
custom_metrics:
  - name: custom.googleapis.com/pipeline/ingestion_rows
    description: "Filas ingestadas por fuente"
    labels: [source, status]
    type: GAUGE
    
  - name: custom.googleapis.com/pipeline/dbt_model_duration
    description: "DuraciÃ³n de modelos dbt en segundos"
    labels: [model, layer, status]
    type: GAUGE
    
  - name: custom.googleapis.com/pipeline/test_results
    description: "Resultados de tests de calidad"
    labels: [test_name, model, status]
    type: GAUGE
    
  - name: custom.googleapis.com/pipeline/freshness_seconds
    description: "Segundos desde Ãºltima actualizaciÃ³n"
    labels: [dataset, table]
    type: GAUGE

# Alerting Policies
alerts:
  - name: pipeline_failure
    condition: >
      metric.type="custom.googleapis.com/pipeline/dbt_model_duration"
      AND metric.label.status="failed"
    duration: 0s
    notification_channels: [pagerduty, slack]
    severity: CRITICAL
    
  - name: freshness_sla_breach
    condition: >
      metric.type="custom.googleapis.com/pipeline/freshness_seconds"
      AND metric.label.dataset STARTS_WITH "gold_"
      AND value > 14400  # 4 hours
    duration: 300s
    notification_channels: [slack]
    severity: WARNING
    
  - name: test_failures
    condition: >
      metric.type="custom.googleapis.com/pipeline/test_results"
      AND metric.label.status="failed"
    duration: 0s
    notification_channels: [slack]
    severity: ERROR
```

### 8.2 Dashboard de Operaciones

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA PLATFORM OPERATIONS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   PIPELINE      â”‚  â”‚   FRESHNESS     â”‚  â”‚   QUALITY       â”‚             â”‚
â”‚  â”‚   STATUS        â”‚  â”‚   SLA           â”‚  â”‚   SCORE         â”‚             â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚             â”‚
â”‚  â”‚   âœ… SUCCESS    â”‚  â”‚   âœ… 2h 15m     â”‚  â”‚   âœ… 98.5%      â”‚             â”‚
â”‚  â”‚   Last: 06:45   â”‚  â”‚   Target: <4h   â”‚  â”‚   52/53 tests   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  PIPELINE DURATION (Last 7 days)                                      â”‚ â”‚
â”‚  â”‚  45m â”¤                                                                â”‚ â”‚
â”‚  â”‚      â”‚    â•­â”€â•®                     â•­â”€â•®                                 â”‚ â”‚
â”‚  â”‚  40m â”¤    â”‚ â”‚  â•­â”€â•®               â”‚ â”‚    â•­â”€â•®                          â”‚ â”‚
â”‚  â”‚      â”‚â•­â”€â•® â”‚ â”‚  â”‚ â”‚ â•­â”€â•®     â•­â”€â•®   â”‚ â”‚    â”‚ â”‚                          â”‚ â”‚
â”‚  â”‚  35m â”¤â”‚ â•°â”€â•¯ â•°â”€â”€â•¯ â•°â”€â•¯ â•°â”€â”€â”€â”€â”€â•¯ â•°â”€â”€â”€â•¯ â•°â”€â”€â”€â”€â•¯ â”‚                          â”‚ â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚ â”‚
â”‚  â”‚       Mon  Tue  Wed  Thu  Fri  Sat  Sun                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ROWS PROCESSED BY LAYER                                              â”‚ â”‚
â”‚  â”‚                                                                       â”‚ â”‚
â”‚  â”‚  Bronze  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  1.2M rows          â”‚ â”‚
â”‚  â”‚  Silver  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        980K rows          â”‚ â”‚
â”‚  â”‚  Gold    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        450K rows          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  INGESTION BY SOURCE            â”‚  â”‚  TOP SLOW MODELS                â”‚ â”‚
â”‚  â”‚                                 â”‚  â”‚                                 â”‚ â”‚
â”‚  â”‚  Salesforce   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  450K    â”‚  â”‚  mart_clientes_360    8m 23s   â”‚ â”‚
â”‚  â”‚  PostgreSQL   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 520K   â”‚  â”‚  fct_transacciones    6m 45s   â”‚ â”‚
â”‚  â”‚  SAP          â–ˆâ–ˆâ–ˆâ–ˆ       180K   â”‚  â”‚  agg_ventas_diarias   4m 12s   â”‚ â”‚
â”‚  â”‚  SurveyMonkey â–ˆâ–ˆ          50K   â”‚  â”‚  dim_clientes         3m 56s   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.3 Alertas y Runbooks

| Alerta | Severidad | Canal | Runbook |
|--------|-----------|-------|---------|
| Pipeline failed | P1 - Critical | PagerDuty | [runbook/pipeline-failure.md](runbook/pipeline-failure.md) |
| Freshness SLA breach | P2 - High | Slack #data-alerts | [runbook/freshness-breach.md](runbook/freshness-breach.md) |
| dbt test failed | P2 - High | Slack #data-alerts | [runbook/test-failure.md](runbook/test-failure.md) |
| Airbyte sync failed | P2 - High | Slack #data-alerts | [runbook/airbyte-failure.md](runbook/airbyte-failure.md) |
| High query latency | P3 - Medium | Slack #data-engineering | [runbook/query-performance.md](runbook/query-performance.md) |
| Storage quota warning | P3 - Medium | Email | [runbook/storage-quota.md](runbook/storage-quota.md) |

---

## 9. Infrastructure as Code

### 9.1 Estructura de Terraform

```
terraform/
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ terraform.tfvars
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ prod/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ bigquery/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”‚   â””â”€â”€ datasets.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ gke/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ node_pools.tf
â”‚   â”‚   â””â”€â”€ workload_identity.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ composer/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ dags_bucket.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ iam/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ groups.tf
â”‚   â”‚   â”œâ”€â”€ service_accounts.tf
â”‚   â”‚   â””â”€â”€ bindings.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ networking/
â”‚   â”‚   â”œâ”€â”€ vpc.tf
â”‚   â”‚   â”œâ”€â”€ subnets.tf
â”‚   â”‚   â””â”€â”€ firewall.tf
â”‚   â”‚
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ dashboards.tf
â”‚       â”œâ”€â”€ alerts.tf
â”‚       â””â”€â”€ log_sinks.tf
â”‚
â”œâ”€â”€ backend.tf
â””â”€â”€ versions.tf
```

### 9.2 MÃ³dulo BigQuery (Ejemplo)

```hcl
# modules/bigquery/main.tf

variable "project_id" {
  type = string
}

variable "region" {
  type    = string
  default = "US"
}

variable "datasets" {
  type = map(object({
    description = string
    labels      = map(string)
    access      = list(object({
      role          = string
      user_by_email = optional(string)
      group_by_email = optional(string)
      special_group = optional(string)
    }))
  }))
}

# Dataset Bronze
resource "google_bigquery_dataset" "bronze" {
  dataset_id  = "bronze"
  project     = var.project_id
  location    = var.region
  description = "Raw data layer - immutable source data"
  
  labels = {
    layer       = "bronze"
    environment = var.environment
    managed_by  = "terraform"
  }
  
  default_table_expiration_ms = null  # No expiration for bronze
  
  access {
    role          = "OWNER"
    special_group = "projectOwners"
  }
  
  access {
    role          = "WRITER"
    user_by_email = google_service_account.airbyte.email
  }
  
  access {
    role           = "READER"
    group_by_email = "grp-data-engineering@${var.domain}"
  }
}

# Dataset Silver
resource "google_bigquery_dataset" "silver" {
  dataset_id  = "silver"
  project     = var.project_id
  location    = var.region
  description = "Cleaned and conformed data layer"
  
  labels = {
    layer       = "silver"
    environment = var.environment
    managed_by  = "terraform"
  }
  
  default_table_expiration_ms = 31536000000  # 365 days
  
  access {
    role          = "OWNER"
    special_group = "projectOwners"
  }
  
  access {
    role          = "WRITER"
    user_by_email = google_service_account.dbt.email
  }
  
  access {
    role           = "READER"
    group_by_email = "grp-data-engineering@${var.domain}"
  }
  
  access {
    role           = "READER"
    group_by_email = "grp-data-scientists@${var.domain}"
  }
}

# Datasets Gold (dinÃ¡micos)
resource "google_bigquery_dataset" "gold" {
  for_each = toset(["clientes", "productos", "ventas"])
  
  dataset_id  = "gold_${each.key}"
  project     = var.project_id
  location    = var.region
  description = "Business marts for ${each.key} domain"
  
  labels = {
    layer       = "gold"
    domain      = each.key
    environment = var.environment
    managed_by  = "terraform"
  }
  
  access {
    role          = "OWNER"
    special_group = "projectOwners"
  }
  
  access {
    role          = "WRITER"
    user_by_email = google_service_account.dbt.email
  }
  
  access {
    role           = "READER"
    group_by_email = "grp-domain-${each.key}@${var.domain}"
  }
  
  access {
    role           = "READER"
    group_by_email = "grp-data-engineering@${var.domain}"
  }
}

# Policy Tags para Column-Level Security
resource "google_data_catalog_taxonomy" "security" {
  project      = var.project_id
  region       = var.region
  display_name = "Data Security Classification"
  description  = "Taxonomy for data classification and access control"
  
  activated_policy_types = ["FINE_GRAINED_ACCESS_CONTROL"]
}

resource "google_data_catalog_policy_tag" "pii" {
  taxonomy     = google_data_catalog_taxonomy.security.id
  display_name = "PII"
  description  = "Personally Identifiable Information"
}

resource "google_data_catalog_policy_tag" "financial" {
  taxonomy     = google_data_catalog_taxonomy.security.id
  display_name = "Financial"
  description  = "Sensitive financial data"
}
```

### 9.3 CI/CD Pipeline (Cloud Build)

```yaml
# cloudbuild.yaml
steps:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # TERRAFORM VALIDATION
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  - id: 'tf-init'
    name: 'hashicorp/terraform:1.6'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        cd terraform/environments/${_ENVIRONMENT}
        terraform init -backend-config="bucket=${_TF_STATE_BUCKET}"
    
  - id: 'tf-validate'
    name: 'hashicorp/terraform:1.6'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        cd terraform/environments/${_ENVIRONMENT}
        terraform validate
    waitFor: ['tf-init']
    
  - id: 'tf-plan'
    name: 'hashicorp/terraform:1.6'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        cd terraform/environments/${_ENVIRONMENT}
        terraform plan -out=tfplan -var-file=terraform.tfvars
    waitFor: ['tf-validate']

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # DBT VALIDATION
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  - id: 'dbt-deps'
    name: 'ghcr.io/dbt-labs/dbt-bigquery:1.7.0'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        cd dbt/maisons
        dbt deps
    waitFor: ['-']
    
  - id: 'dbt-compile'
    name: 'ghcr.io/dbt-labs/dbt-bigquery:1.7.0'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        cd dbt/maisons
        dbt compile --target ci
    waitFor: ['dbt-deps']
    
  - id: 'dbt-test-unit'
    name: 'ghcr.io/dbt-labs/dbt-bigquery:1.7.0'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        cd dbt/maisons
        dbt test --select test_type:unit --target ci
    waitFor: ['dbt-compile']

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # DAG VALIDATION
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  - id: 'dag-lint'
    name: 'python:3.11-slim'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        pip install apache-airflow==2.7.0 pylint
        cd dags
        python -m py_compile *.py
        pylint --disable=all --enable=E *.py
    waitFor: ['-']

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # DEPLOY (only on main branch)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  - id: 'tf-apply'
    name: 'hashicorp/terraform:1.6'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        if [ "${BRANCH_NAME}" = "main" ]; then
          cd terraform/environments/${_ENVIRONMENT}
          terraform apply -auto-approve tfplan
        fi
    waitFor: ['tf-plan', 'dbt-test-unit', 'dag-lint']
    
  - id: 'sync-dags'
    name: 'gcr.io/cloud-builders/gsutil'
    args:
      - '-m'
      - 'rsync'
      - '-r'
      - '-d'
      - 'dags/'
      - 'gs://${_COMPOSER_BUCKET}/dags/'
    waitFor: ['tf-apply']
    
  - id: 'sync-dbt'
    name: 'gcr.io/cloud-builders/gsutil'
    args:
      - '-m'
      - 'rsync'
      - '-r'
      - '-d'
      - 'dbt/'
      - 'gs://${_COMPOSER_BUCKET}/dbt/'
    waitFor: ['tf-apply']

substitutions:
  _ENVIRONMENT: 'prod'
  _TF_STATE_BUCKET: 'maisons-terraform-state'
  _COMPOSER_BUCKET: 'us-central1-maisons-composer-abc123-bucket'

options:
  logging: CLOUD_LOGGING_ONLY
```

---

## 10. Estrategia de Testing

### 10.1 PirÃ¡mide de Tests

```
                    â•±â•²
                   â•±  â•²
                  â•± E2Eâ•²         Integration tests (weekly)
                 â•±â”€â”€â”€â”€â”€â”€â•²        - Full pipeline execution
                â•±        â•²       - Data reconciliation
               â•±Integrationâ•²     
              â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²     Contract tests (on merge)
             â•±              â•²    - Schema validation
            â•±   Contract     â•²   - API compatibility
           â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²  
          â•±                    â•² Unit tests (on commit)
         â•±       Unit          â•² - dbt tests
        â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•² - SQL logic validation
       â•±                        â•²
      â•±        Static           â•² Linting (on commit)
     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•² - SQL formatting
    â•±                            â•² - DAG validation
   â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
```

### 10.2 Tests dbt por Capa

```yaml
# models/staging/_staging__models.yml
version: 2

models:
  - name: stg_clientes
    description: "Clientes limpiados desde Salesforce"
    columns:
      - name: cliente_id
        description: "Identificador Ãºnico del cliente"
        tests:
          - not_null
          - unique
      - name: email
        tests:
          - not_null
          - unique:
              config:
                severity: warn
      - name: fecha_registro
        tests:
          - not_null
          - dbt_utils.not_in_future

  - name: stg_ventas
    columns:
      - name: venta_id
        tests:
          - not_null
          - unique
      - name: cliente_id
        tests:
          - not_null
          - relationships:
              to: ref('stg_clientes')
              field: cliente_id
      - name: monto
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              inclusive: true

# models/marts/clientes/_clientes__models.yml
version: 2

models:
  - name: mart_clientes_360
    description: "Vista 360 del cliente con mÃ©tricas agregadas"
    tests:
      - dbt_utils.equal_rowcount:
          compare_model: ref('stg_clientes')
    columns:
      - name: cliente_id
        tests:
          - not_null
          - unique
      - name: segmento_valor
        tests:
          - accepted_values:
              values: ['premium', 'standard', 'basic']
      - name: ltv_score
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 1000000
      - name: total_compras
        tests:
          - dbt_utils.expression_is_true:
              expression: ">= 0"
      - name: total_gastado
        tests:
          - dbt_utils.expression_is_true:
              expression: ">= 0"
```

### 10.3 Great Expectations Checkpoint

```yaml
# great_expectations/checkpoints/gold_checkpoint.yml
name: gold_checkpoint
config_version: 1.0
class_name: Checkpoint
run_name_template: "%Y%m%d-%H%M%S-gold-validation"

validations:
  - batch_request:
      datasource_name: bigquery_gold
      data_connector_name: default_inferred_data_connector_name
      data_asset_name: gold_clientes.mart_clientes_360
    expectation_suite_name: mart_clientes_360_suite

  - batch_request:
      datasource_name: bigquery_gold
      data_connector_name: default_inferred_data_connector_name
      data_asset_name: gold_ventas.fct_transacciones
    expectation_suite_name: fct_transacciones_suite

action_list:
  - name: store_validation_result
    action:
      class_name: StoreValidationResultAction
      
  - name: update_data_docs
    action:
      class_name: UpdateDataDocsAction
      
  - name: send_slack_notification
    action:
      class_name: SlackNotificationAction
      slack_webhook: ${SLACK_WEBHOOK}
      notify_on: failure
      renderer:
        module_name: great_expectations.render.renderer.slack_renderer
        class_name: SlackRenderer
```

---

## 11. Performance y Escalabilidad

### 11.1 Estrategias de OptimizaciÃ³n por Capa

| Capa | Estrategia | ImplementaciÃ³n |
|------|------------|----------------|
| **Bronze** | Partitioning | `_airbyte_extracted_at` (DAY) |
| **Bronze** | Clustering | `_airbyte_source_id` |
| **Silver** | Incremental models | `is_incremental()` en dbt |
| **Silver** | Partitioning | `loaded_at` (DAY) |
| **Gold** | Materialized views | Para agregaciones frecuentes |
| **Gold** | Clustering | Columnas de filtro frecuente |
| **General** | Slot reservations | Para workloads predecibles |

### 11.2 ConfiguraciÃ³n de BigQuery

```hcl
# Reservation para workloads de transformaciÃ³n
resource "google_bigquery_reservation" "dbt_slots" {
  name     = "dbt-transformation"
  project  = var.project_id
  location = var.region
  
  slot_capacity     = 100
  ignore_idle_slots = false
}

resource "google_bigquery_reservation_assignment" "dbt_assignment" {
  assignee    = "projects/${var.project_id}"
  job_type    = "QUERY"
  reservation = google_bigquery_reservation.dbt_slots.name
}
```

### 11.3 ProyecciÃ³n de Escalabilidad

| Escenario | Volumen | Estrategia | Cambios Requeridos |
|-----------|---------|------------|-------------------|
| **Actual** | 10 GB/dÃ­a | On-demand | Ninguno |
| **2x** | 20 GB/dÃ­a | On-demand + optimizaciÃ³n | Clustering adicional |
| **5x** | 50 GB/dÃ­a | Flat-rate slots (100) | Reservations |
| **10x** | 100 GB/dÃ­a | Flat-rate slots (200) | MÃ¡s workers Composer |
| **50x** | 500 GB/dÃ­a | Enterprise slots | RediseÃ±o de particiones |

### 11.4 Modelo Incremental Optimizado

```sql
-- Ejemplo de modelo incremental con merge strategy
{{
    config(
        materialized='incremental',
        unique_key='transaccion_id',
        incremental_strategy='merge',
        partition_by={
            'field': 'fecha_venta',
            'data_type': 'date',
            'granularity': 'day'
        },
        cluster_by=['canal', 'region'],
        partition_expiration_days=730
    )
}}

WITH source AS (
    SELECT * FROM {{ ref('stg_ventas') }}
    {% if is_incremental() %}
    WHERE loaded_at > (
        SELECT COALESCE(MAX(source_loaded_at), '1900-01-01')
        FROM {{ this }}
    )
    {% endif %}
),

transformed AS (
    SELECT
        transaccion_id,
        cliente_id,
        producto_id,
        DATE(fecha_transaccion) AS fecha_venta,
        monto,
        cantidad,
        canal,
        region,
        metodo_pago,
        loaded_at AS source_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at
    FROM source
)

SELECT * FROM transformed
```

---

## 12. EstimaciÃ³n de Costos Detallada

### 12.1 Desglose por Componente

| Componente | Servicio | EspecificaciÃ³n | Costo/Mes |
|------------|----------|----------------|-----------|
| **Ingesta** | GKE (Airbyte) | n2-standard-4 x 2 nodes | $280 |
| | | Persistent disks (100GB) | $20 |
| **OrquestaciÃ³n** | Cloud Composer | Medium environment | $400 |
| | | 3 workers (n1-standard-2) | Incluido |
| **Storage Bronze** | BigQuery | ~100 GB active | $2 |
| | | ~400 GB long-term | $4 |
| **Storage Silver** | BigQuery | ~80 GB active | $1.60 |
| **Storage Gold** | BigQuery | ~30 GB active | $0.60 |
| **Compute** | BigQuery | ~5 TB scanned/month | $25 |
| | | On-demand queries | Variable |
| **Data Lake** | GCS | ~200 GB Standard | $4 |
| **CatÃ¡logo** | GKE (DataHub) | n2-standard-2 x 2 nodes | $140 |
| **BI** | GKE (Superset) | n2-standard-2 x 2 nodes | $140 |
| **ML** | Vertex AI | Training (10 hrs) | $50 |
| | | Endpoints (low traffic) | $30 |
| **Networking** | VPC | Egress ~100 GB | $8 |
| **Monitoring** | Cloud Monitoring | Metrics + Logs | $50 |
| **CI/CD** | Cloud Build | ~100 builds/month | $10 |
| **â”€â”€â”€â”€â”€â”€â”€** | **â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€** | **â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€** | **â”€â”€â”€â”€â”€â”€** |
| **TOTAL** | | | **~$1,165** |

### 12.2 ProyecciÃ³n de Costos por Escala

| Volumen Datos | Storage | Compute | Infra | Total/Mes |
|---------------|---------|---------|-------|-----------|
| 10 GB/dÃ­a | $10 | $25 | $1,130 | ~$1,165 |
| 50 GB/dÃ­a | $50 | $125 | $1,130 | ~$1,305 |
| 100 GB/dÃ­a | $100 | $250 | $1,500 | ~$1,850 |
| 500 GB/dÃ­a | $500 | $1,000 | $2,500 | ~$4,000 |

### 12.3 Optimizaciones de Costo

| OptimizaciÃ³n | Ahorro Estimado | ImplementaciÃ³n |
|--------------|-----------------|----------------|
| Committed use discounts (GKE) | 20-30% | Contratos 1-3 aÃ±os |
| BigQuery flat-rate | Variable | >$1000/mes en compute |
| GCS lifecycle policies | 30-40% en storage | Bronze >1 aÃ±o â†’ Nearline |
| Preemptible nodes (dev) | 60-80% | Non-production clusters |
| Scheduled scaling | 20-30% | Reducir workers noche/fin de semana |

---

## 13. Riesgos y Mitigaciones

### 13.1 Matriz de Riesgos

| ID | Riesgo | Probabilidad | Impacto | MitigaciÃ³n |
|----|--------|--------------|---------|------------|
| R1 | Complejidad operacional Airbyte | Media | Alto | Runbooks, capacitaciÃ³n, considerar Airbyte Cloud |
| R2 | Costos BigQuery no controlados | Media | Alto | Quotas por dataset, alertas de billing, slot reservations |
| R3 | Schema drift en fuentes | Alta | Medio | Airbyte auto-schema, tests de schema en dbt |
| R4 | Latencia excesiva Bronzeâ†’Gold | Baja | Alto | Incremental models, optimizaciÃ³n de queries |
| R5 | Calidad de datos en origen | Alta | Alto | Validaciones en Bronze, alertas tempranas, data contracts |
| R6 | Vendor lock-in GCP | Baja | Medio | Usar open-source donde posible, abstraer con dbt |
| R7 | Single point of failure | Media | Alto | HA en Composer, multi-zone GKE, backups |
| R8 | Seguridad/compliance | Baja | CrÃ­tico | Column-level security, auditorÃ­a, encryption |

### 13.2 Plan de Contingencia

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INCIDENT RESPONSE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  P1 (Critical) - Pipeline down                                 â”‚
â”‚  â”œâ”€â”€ Detection: < 5 min (Cloud Monitoring)                     â”‚
â”‚  â”œâ”€â”€ Notification: PagerDuty â†’ On-call engineer                â”‚
â”‚  â”œâ”€â”€ Response: < 15 min                                        â”‚
â”‚  â””â”€â”€ Resolution: < 2 hours                                     â”‚
â”‚                                                                 â”‚
â”‚  P2 (High) - Data quality issue                                â”‚
â”‚  â”œâ”€â”€ Detection: < 30 min (dbt tests)                           â”‚
â”‚  â”œâ”€â”€ Notification: Slack #data-alerts                          â”‚
â”‚  â”œâ”€â”€ Response: < 1 hour                                        â”‚
â”‚  â””â”€â”€ Resolution: < 4 hours                                     â”‚
â”‚                                                                 â”‚
â”‚  P3 (Medium) - Performance degradation                         â”‚
â”‚  â”œâ”€â”€ Detection: < 1 hour (metrics)                             â”‚
â”‚  â”œâ”€â”€ Notification: Slack #data-engineering                     â”‚
â”‚  â”œâ”€â”€ Response: Next business day                               â”‚
â”‚  â””â”€â”€ Resolution: < 1 week                                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 14. Roadmap de ImplementaciÃ³n

### 14.1 Timeline Detallado

```
Phase 1: Foundation (Weeks 1-4)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
W1  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Terraform setup, GCP project, networking
W2  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ GKE cluster, Airbyte deployment
W3  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Cloud Composer, BigQuery datasets
W4  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ IAM, service accounts, secrets

Phase 2: First Sources (Weeks 5-8)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
W5  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Airbyte: PostgreSQL + Salesforce connectors
W6  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ dbt: Bronze sources, staging models
W7  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ dbt: Silver models, intermediate
W8  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Testing framework, CI/CD pipeline

Phase 3: Gold Layer (Weeks 9-12)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
W9  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ gold_clientes: dims + facts + marts
W10 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Quality tests, Column-level security
W11 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Superset: Connection + first dashboards
W12 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Documentation, DataHub setup

Phase 4: Full Coverage (Weeks 13-16)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
W13 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Airbyte: SAP, MySQL, MongoDB, SurveyMonkey
W14 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ gold_productos: complete implementation
W15 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ gold_ventas: complete implementation
W16 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Cross-domain marts, lineage validation

Phase 5: ML & Optimization (Weeks 17-20)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
W17 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Vertex AI: Feature Store setup
W18 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ First ML model (churn prediction)
W19 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Performance optimization, cost tuning
W20 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Handover, training, go-live
```

### 14.2 Entregables por Fase

| Fase | Entregables | Criterio de Ã‰xito |
|------|-------------|-------------------|
| **1** | Infra base desplegada | Terraform apply exitoso, todos los servicios healthy |
| **2** | 2 fuentes en Bronze | Syncs diarios exitosos, datos en silver |
| **3** | gold_clientes productivo | Dashboard funcionando, tests passing |
| **4** | 6 fuentes, 3 Gold datasets | Pipeline completo < 2 horas |
| **5** | ML en producciÃ³n | Modelo desplegado, predicciones diarias |

### 14.3 Equipo Requerido

| Rol | DedicaciÃ³n | Responsabilidades |
|-----|------------|-------------------|
| Data Engineer Lead | 100% | Arquitectura, Airbyte, pipelines |
| Data Engineer | 100% | dbt models, testing, CI/CD |
| Analytics Engineer | 50% | Gold marts, Superset dashboards |
| DevOps/Platform | 25% | Terraform, GKE, monitoring |
| ML Engineer | 25% (fase 5) | Vertex AI, feature engineering |

---

## Anexos

### Anexo A: Checklist de Go-Live

- [ ] Todos los tests de dbt pasan en producciÃ³n
- [ ] Freshness SLA validado por 5 dÃ­as consecutivos
- [ ] Column-level security verificado
- [ ] Runbooks documentados y probados
- [ ] Alertas configuradas y validadas
- [ ] Backups verificados
- [ ] CapacitaciÃ³n de usuarios completada
- [ ] DocumentaciÃ³n en DataHub completa

### Anexo B: Contactos y EscalaciÃ³n

| Nivel | Contacto | Canal | SLA Respuesta |
|-------|----------|-------|---------------|
| L1 | Data Engineering | Slack #data-support | 4 horas |
| L2 | Data Platform Lead | PagerDuty | 1 hora |
| L3 | CTO / Head of Data | TelÃ©fono | 30 minutos |

### Anexo C: Referencias TÃ©cnicas

- [dbt Best Practices](https://docs.getdbt.com/guides/best-practices)
- [BigQuery Optimization](https://cloud.google.com/bigquery/docs/best-practices-performance-overview)
- [Airbyte Documentation](https://docs.airbyte.com/)
- [Data Mesh Principles](https://martinfowler.com/articles/data-mesh-principles.html)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)

---

*Documento generado para revisiÃ³n tÃ©cnica. VersiÃ³n 2.0*
